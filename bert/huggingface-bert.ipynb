{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "huggingface-bert.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KljpE8gRG2mm"
      },
      "source": [
        "# https://github.com/onnx/tensorflow-onnx/blob/master/tutorials/huggingface-bert.ipynb"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk72uSN5dm-K"
      },
      "source": [
        "# Converting a Huggingface model to ONNX with tf2onnx\n",
        "\n",
        "This is a simple example how to convert a [huggingface](https://huggingface.co/) model to ONNX using [tf2onnx](https://github.com/onnx/tensorflow-onnx).\n",
        "\n",
        "We use the [TFBertForQuestionAnswering](https://huggingface.co/transformers/model_doc/bert.html#tfbertforquestionanswering) example from huggingface.\n",
        "\n",
        "Other models will work similar. You'll find additional examples for other models in our unit tests [here](https://github.com/onnx/tensorflow-onnx/blob/master/tests/huggingface.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l-m7VrHdm-N"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyEVYvR2dm-N",
        "outputId": "8c89b40c-2f1f-4b30-9f97-887030000939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow transformers tf2onnx onnxruntime"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.7/dist-packages (1.9.3)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.10.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe5fCKqJdm-P"
      },
      "source": [
        "## The keras code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jto-Lbgxdm-P"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import onnxruntime as rt\n",
        "import tensorflow as tf\n",
        "import tf2onnx\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from tokenizers import BertWordPieceTokenizer"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOOmfGhSgVX-"
      },
      "source": [
        ""
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mA1qKoHFAim",
        "outputId": "7ca23164-ff39-42d6-c76e-4d74dcc11cc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-06 05:55:14--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.201.104\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.201.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231508 (226K) [text/plain]\n",
            "Saving to: ‘bert-base-uncased-vocab.txt.1’\n",
            "\n",
            "bert-base-uncased-v 100%[===================>] 226.08K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-12-06 05:55:15 (4.07 MB/s) - ‘bert-base-uncased-vocab.txt.1’ saved [231508/231508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVsUEtFaDwmv",
        "outputId": "aabe7820-5942-4e75-fcce-d0de1f84c880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "max_length=512\n",
        "tokenizer = BertWordPieceTokenizer('bert-base-uncased-vocab.txt', lowercase=False)\n",
        "tokenizer.enable_truncation(max_length=max_length)\n",
        "tokenizer.enable_padding(length=max_length)\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=1)\n",
        "\n",
        "def fast_tokenize(texts, chunk_size=512):\n",
        "    all_ids, all_attentions, all_tokens = [], [], []\n",
        "    for i in range(0, len(texts), chunk_size):\n",
        "        text_chunk = texts[i : i + chunk_size]#.tolist()\n",
        "        encs = tokenizer.encode_batch(text_chunk)\n",
        "        all_ids.extend([enc.ids for enc in encs])\n",
        "        all_attentions.extend([enc.attention_mask for enc in encs])\n",
        "        all_tokens.extend([enc.type_ids for enc in encs])\n",
        "    return (\n",
        "        tf.convert_to_tensor(all_ids),\n",
        "        tf.convert_to_tensor(all_attentions),\n",
        "        tf.convert_to_tensor(all_tokens),\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r-GjXLYE_xh"
      },
      "source": [
        "texts = [\"\"\"\n",
        "\"WHAT A GREAT PLACE FOR YOUR BEACH VACATION! \\n\\nMy 2 bedroom/2 bath condo is directly across from the Beach Club, the beach. The condo has been completely remodeled with new ceramic flooring and countertops in kitchen and baths, new carpet, new lighting fixtures and appliances. \\n\\nThe relaxing top floor view from the rear screened porch (where smoking is permitted) is of the marsh and tidal creek and is a great place to start your day with coffee. There is a large deck overhanging the canal for crabbing and fishing and it is a great place to have a glass of wine and grill a steak at the end of the day.\\nYou are likely coming to Fripp for 3 miles of ocean front white sand beach across the street. There are numerous bike trails and 2 adult bicycles are furnished. A golf cart is furnished and this is the way guests prefer to see the island.. Fripp Island is a wildlife preserve. No where else will you find over 300 docile deer, allowing many photo opportunities. In addition there is various wildlife, birds and water foul and alligators. Walk the educational Audubon Trail and visit Hunting Island State Park across the bridge from Fripp. Hunting Island has many bike trails and a light house children enjoy climbing. Dont miss the opportunity to Visit Bay Street in historic Beaufort where great food, relaxing views and shopping are yours to enjoy.\\n\\n\\nKeywords: Condominium\"\t\n",
        "\"\"\"]\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOgnf-MjFj4K",
        "outputId": "86a29095-d464-42fc-b55b-92401a9ed984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# %%timeit\n",
        "tokenized_text = fast_tokenize(texts)\n",
        "tf_results = model.predict(tokenized_text, verbose=1)\n",
        "# tf_results = tf.nn.softmax(scores.logits, axis=1)#[:, 1]#.numpy()\n",
        "tf_results"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFSequenceClassifierOutput([('logits', array([[0.48259255]], dtype=float32))])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY6c_qq9dm-R"
      },
      "source": [
        "## Convert to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48S_C9pydm-S"
      },
      "source": [
        "# describe the inputs\n",
        "input_spec = (\n",
        "    tf.TensorSpec((None,  None), tf.int32, name=\"input_ids\"),\n",
        "    tf.TensorSpec((None,  None), tf.int32, name=\"token_type_ids\"),\n",
        "    tf.TensorSpec((None,  None), tf.int32, name=\"attention_mask\")\n",
        ")\n",
        "\n",
        "# and convert\n",
        "_, _ = tf2onnx.convert.from_keras(model, input_signature=input_spec, opset=13, output_path=\"bert.onnx\")"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjB_WsvkIsTM",
        "outputId": "945892d2-4392-404e-dc5f-5583da4a391e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -hs /content/bert.onnx"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "414M\t/content/bert.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TP_n9djdm-T"
      },
      "source": [
        "## Test the ONNX model with onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn4D9jtbSRAM"
      },
      "source": [
        "opt = rt.SessionOptions()\n",
        "ort_session = rt.InferenceSession(\"bert.onnx\")"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Tt00v5dm-U",
        "outputId": "c1267762-b41c-4cac-ac9f-dc8ce5275830",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# %%timeit\n",
        "tokenized_text = fast_tokenize(texts)\n",
        "input_ids=tokenized_text[0]\n",
        "attention_masks = tokenized_text[1]\n",
        "all_tokens = tokenized_text[2]\n",
        "\n",
        "\n",
        "input_dict = {\"input_ids\" : input_ids.numpy() , \"attention_mask\" : attention_masks.numpy() , \"token_type_ids\": all_tokens.numpy() }\n",
        "onnx_results = ort_session.run(None, input_dict)\n",
        "onnx_results"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.48259258]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0nyMupedm-V"
      },
      "source": [
        "## Make sure tensorflow and onnxruntime results are the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYj0xRfUdm-V"
      },
      "source": [
        "# outputs are matching\n",
        "np.testing.assert_allclose(tf_results['logits'], onnx_results[0], rtol=1e-5, atol=1e-5)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG4ymhI4S9cr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}